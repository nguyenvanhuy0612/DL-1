{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "# Load training data from file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split(\"\\n\")\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(\" \", 1)\n",
    "        label = int(label.split(\"__label__\")[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({\"label\": labels, \"text\": texts})\n",
    "\n",
    "\n",
    "train_filepath = \"train.3270.txt\"\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"label\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_vocab_size = 20000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          2000000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,117,377\n",
      "Trainable params: 2,117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=max_vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_sequence_length,\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 4s 43ms/step - loss: -6.5737 - accuracy: 0.5417 - val_loss: -11.1879 - val_accuracy: 0.5229\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 3s 40ms/step - loss: -13.0926 - accuracy: 0.5459 - val_loss: -16.3399 - val_accuracy: 0.5229\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -17.9304 - accuracy: 0.5459 - val_loss: -21.1895 - val_accuracy: 0.5229\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -22.4540 - accuracy: 0.5459 - val_loss: -25.9740 - val_accuracy: 0.5229\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 3s 41ms/step - loss: -26.9737 - accuracy: 0.5459 - val_loss: -30.7127 - val_accuracy: 0.5229\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -31.4826 - accuracy: 0.5459 - val_loss: -35.3748 - val_accuracy: 0.5229\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 3s 41ms/step - loss: -35.7278 - accuracy: 0.5459 - val_loss: -40.0208 - val_accuracy: 0.5229\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -40.2528 - accuracy: 0.5459 - val_loss: -44.6525 - val_accuracy: 0.5229\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -44.6904 - accuracy: 0.5459 - val_loss: -49.3284 - val_accuracy: 0.5229\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 3s 39ms/step - loss: -49.1346 - accuracy: 0.5459 - val_loss: -53.8867 - val_accuracy: 0.5229\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val_padded, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 52.29%\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "[[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict on new data\n",
    "new_texts = [\"This product is great!\", \"I did not like this book at all.\"]\n",
    "new_texts_preprocessed = [preprocess_text(text) for text in new_texts]\n",
    "new_texts_sequences = tokenizer.texts_to_sequences(new_texts_preprocessed)\n",
    "new_texts_padded = pad_sequences(new_texts_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "predictions = model.predict(new_texts_padded)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    1770\n",
      "2    1500\n",
      "Name: count, dtype: int64\n",
      "First text: great basic midi understanding setup student musician institute la wanted learn midi bought book old picture couple section date able tell difference today computer used music fine great explanation easy discription midi message sent use put together midi system great book helped lot\n",
      "Terms and TF-IDF scores for the first text:\n",
      "(12900, wanted)\t0.10354144891282427\n",
      "(12628, used)\t0.08944348644565366\n",
      "(12626, use)\t0.0831250697453042\n",
      "(12428, understanding)\t0.13080635575538221\n",
      "(12041, together)\t0.10710789930549768\n",
      "(12036, today)\t0.1113371798815985\n",
      "(11772, tell)\t0.09943254464245621\n",
      "(11661, system)\t0.1255186695640299\n",
      "(11393, student)\t0.12327312776395585\n",
      "(10518, setup)\t0.16561761829137742\n",
      "(10473, sent)\t0.12474533255604606\n",
      "(10412, section)\t0.12890431644477274\n",
      "(9390, put)\t0.08655982591735115\n",
      "(8770, picture)\t0.09968460529551991\n",
      "(8243, old)\t0.08613607521190975\n",
      "(7875, musician)\t0.1390846862387611\n",
      "(7871, music)\t0.08627645262107235\n",
      "(7599, midi)\t0.6828312127534865\n",
      "(7564, message)\t0.12890431644477274\n",
      "(7168, lot)\t0.08758030788890649\n",
      "(6903, learn)\t0.11499479728057696\n",
      "(6774, la)\t0.1405568910308513\n",
      "(6308, institute)\t0.17070780318837162\n",
      "(5701, helped)\t0.14980605981657216\n",
      "(5403, great)\t0.17784537411834694\n",
      "(4747, fine)\t0.10607864546034823\n",
      "(4465, explanation)\t0.1476319148103083\n",
      "(3944, easy)\t0.09705421994083231\n",
      "(3565, discription)\t0.1865193616631769\n",
      "(3455, difference)\t0.12800964267729711\n",
      "(3109, date)\t0.12890431644477274\n",
      "(2858, couple)\t0.11010383860190864\n",
      "(2578, computer)\t0.10607864546034823\n",
      "(1614, bought)\t0.07898653521428975\n",
      "(1564, book)\t0.09692720048388818\n",
      "(1229, basic)\t0.11996134362556896\n",
      "(333, able)\t0.10446014717646508\n",
      "Validation Accuracy: 0.8058103975535168\n",
      "Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.87      0.82       342\n",
      "           2       0.84      0.74      0.78       312\n",
      "\n",
      "    accuracy                           0.81       654\n",
      "   macro avg       0.81      0.80      0.80       654\n",
      "weighted avg       0.81      0.81      0.80       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "# Function to preprocess data from text file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split(\"\\n\")\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(\" \", 1)\n",
    "        label = int(label.split(\"__label__\")[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({\"label\": labels, \"text\": texts})\n",
    "\n",
    "\n",
    "# Load training data from file\n",
    "train_filepath = \"train.3270.txt\"\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Initialize stop words, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text, method=\"lemmatize\"):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    if method == \"stem\":\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif method == \"lemmatize\":\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Apply text preprocessing\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(\n",
    "    lambda x: preprocess_text(x, method=\"lemmatize\")\n",
    ")\n",
    "\n",
    "# Check for imbalanced data\n",
    "print(df_train[\"label\"].value_counts())\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"label\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# Show the first text converted to numerical form\n",
    "first_text = X_train.iloc[0]\n",
    "first_text_tfidf = vectorizer.transform([first_text])\n",
    "print(f\"First text: {first_text}\")\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the terms with their corresponding TF-IDF scores for the first text\n",
    "print(\"Terms and TF-IDF scores for the first text:\")\n",
    "for index, score in zip(first_text_tfidf.indices, first_text_tfidf.data):\n",
    "    term = feature_names[index]\n",
    "    print(f\"({index}, {term})\\t{score}\")\n",
    "\n",
    "# Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model evaluation on validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample Data (For demonstration purposes)\n",
    "df_train = pd.DataFrame(\n",
    "    {\n",
    "        \"label\": [1, 2, 1, 2],\n",
    "        \"text\": [\n",
    "            \"I love this product, it's amazing.\",\n",
    "            \"Terrible experience, would not recommend.\",\n",
    "            \"Fantastic quality and great service.\",\n",
    "            \"Worst product ever, completely useless.\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r\"\\W\", \" \", text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Apply text preprocessing\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train[\"text\"]\n",
    "y = df_train[\"label\"]\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# Show the first text converted to numerical form\n",
    "first_text = X_train.iloc[0]\n",
    "first_text_tfidf = vectorizer.transform([first_text])\n",
    "print(f\"First text: {first_text}\")\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the terms with their corresponding TF-IDF scores for the first text\n",
    "print(\"Terms and TF-IDF scores for the first text:\")\n",
    "for index, score in zip(first_text_tfidf.indices, first_text_tfidf.data):\n",
    "    term = feature_names[index]\n",
    "    print(f\"({index}, {term})\\t{score}\")\n",
    "\n",
    "# Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model evaluation on validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huy1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
