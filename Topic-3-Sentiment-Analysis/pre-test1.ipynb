{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data from text file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from file\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train['text'], df_train['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first text converted to numerical form\n",
    "first_text = X_train.iloc[0]\n",
    "first_text_tfidf = vectorizer.transform([first_text])\n",
    "print(f\"First text: {first_text}\")\n",
    "print(f\"First text TF-IDF features: {first_text_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the term corresponding to the index 14256\n",
    "term = feature_names[14256]\n",
    "print(f\"Term corresponding to index 14256: {term}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the terms with their corresponding TF-IDF scores for the first text\n",
    "print(\"Terms and TF-IDF scores for the first text:\")\n",
    "for index, score in zip(first_text_tfidf.indices, first_text_tfidf.data):\n",
    "    term = feature_names[index]\n",
    "    print(f\"({index}, {term})\\t{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation on validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data from file\n",
    "test_filepath = 'test.135.txt'\n",
    "df_test = preprocess_data_from_file(test_filepath)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)\n",
    "\n",
    "# Feature engineering\n",
    "X_test_tfidf = vectorizer.transform(df_test['text'])\n",
    "\n",
    "# Model evaluation on test set\n",
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "y_test_actual = df_test['label'].to_numpy()\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test_actual, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(df_test['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment for a custom input text\n",
    "def predict_custom_text(text):\n",
    "    text = preprocess_text(text)\n",
    "    X = vectorizer.transform([text])\n",
    "    prediction = model.predict(X)\n",
    "    return prediction[0]\n",
    "\n",
    "# Example custom input text\n",
    "custom_text = \"I absolutely love this product! It's fantastic and works great.\"\n",
    "predicted_label = predict_custom_text(custom_text)\n",
    "print(f\"Custom Text Prediction: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Deployment example (Flask app)\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    text = request.json['text']\n",
    "    text = preprocess_text(text)\n",
    "    X = vectorizer.transform([text])\n",
    "    prediction = model.predict(X)\n",
    "    return jsonify({'prediction': int(prediction[0])})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Natural Language Processing is an exciting field!\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [t for t in tokens if t not in stop_words]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "print(tokens)  # Output: ['natural', 'language', 'processing', 'exciting', 'field']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-Step Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing Functions\n",
    "\n",
    "# Initialize stop words, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, method='lemmatize'):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming or Lemmatization\n",
    "    if method == 'stem':\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif method == 'lemmatize':\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df_train['text'] = df_train['text'].apply(lambda x: preprocess_text(x, method='lemmatize'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Handling Imbalanced Data\n",
    "\n",
    "# Before handling imbalanced data, let's check the distribution of labels.\n",
    "print(df_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is imbalanced, we can use SMOTE to oversample the minority class.\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# Handle imbalanced data using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_tfidf_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Training and Evaluation\n",
    "\n",
    "# Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf_resampled, y_train_resampled)\n",
    "\n",
    "# Model evaluation on validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
