{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efaeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install gensim \"scipy<1.13\" -y\n",
    "\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
    "import pickle\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2138fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HUYNGUYEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'(?:https?:\\/\\/(?:www\\.|(?!www))|www\\.)[\\w\\-\\.\\+/?=&#@:%~]+', 'URL', text)\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load data\n",
    "def preprocess_data_from_file(filepath):\n",
    "    # Read the file\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text.strip())\n",
    "    \n",
    "    # Parallelize the preprocessing of text\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    processed_texts = Parallel(n_jobs=num_cores)(delayed(preprocess_text)(text) for text in texts)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'label': labels, 'text': processed_texts})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "del lemmatizer, stop_words, stopwords, train_filepath, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_.astype(str)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
