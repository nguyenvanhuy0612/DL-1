{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n",
    "![image.png](img/train.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text = df_train['text'][0]\n",
    "print(first_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex: **[\\W_]+**  \n",
    "![image.png](img/regex.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"[\\W_]+\", first_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text_lower = first_text.lower()\n",
    "first_text_word_only = re.sub(r\"[\\W_]+\", ' ', first_text_lower)\n",
    "print(first_text)\n",
    "print(first_text_word_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words: ', len(stop_words))\n",
    "print(stop_words)\n",
    "\n",
    "all_words = first_text_word_only.split()\n",
    "print('all words: ', len(all_words))\n",
    "print(all_words)\n",
    "\n",
    "list_words = []\n",
    "for word in all_words:\n",
    "    if (word not in stop_words):\n",
    "        list_words.append(word)\n",
    "\n",
    "print('after remove stopwords: ',len(list_words))\n",
    "print(list_words)\n",
    "print()\n",
    "\n",
    "print(first_text)\n",
    "print(first_text_word_only)\n",
    "print(' '.join(list_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-line code\n",
    "\" \".join([x for x in re.sub(r\"[\\W_]+\", ' ', df_train['text'][0].lower()).split() if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(list_words) + \"\\n\")\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "first_text_stemming = [stemmer.stem(word) for word in list_words]\n",
    "print(' '.join(first_text_stemming))\n",
    "\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "first_text_lemmantizer = [lemmatizer.lemmatize(word) for word in list_words]\n",
    "print(' '.join(first_text_lemmantizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del first_text, first_text_lower, first_text_word_only, first_text_stemming, first_text_lemmantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from file and preprocess text\n",
    "def preprocess_data_and_text_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        text = preprocess_text(text) #preprocess_text\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_and_text_from_file(train_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Imbalanced Data\n",
    "\n",
    "# Before handling imbalanced data, let's check the distribution of labels.\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Embedding](img/word_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BOW example](img/Example_BoW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load training data from file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "max_vocab_size = 20000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "# Print the word vocabulary\n",
    "print(f\"word_counts: {tokenizer.word_counts}\\n\")\n",
    "print(f\"word_index: {tokenizer.word_index}\\n{len(tokenizer.word_index)}\\n\")\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "print(f\"X_text[0]: {X.to_list()[0]}\\n\")\n",
    "print(f\"X_sequences[0]: {X_sequences[0]}\\n\")\n",
    "\n",
    "# Calculate maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in X_sequences)\n",
    "print(f\"Maximum sequence length in the dataset: {max_sequence_length}\\n\")\n",
    "\n",
    "# Pad sequences\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post')\n",
    "print(f\"X_padded[0]: {X_padded[0]}\\n{len(X_padded[0])}\\n\")\n",
    "\n",
    "# Split padded sequences and labels into training and validation sets\n",
    "X_train_padded, X_val_padded, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and pad sequences\n",
    "def tokenize_and_pad_sequences(X_train, X_val, max_vocab_size=20000, max_sequence_length=None):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Convert texts to sequences\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "    \n",
    "    # Determine maximum sequence length\n",
    "    max_length = max(len(seq) for seq in X_train_sequences)\n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max_length\n",
    "    else:\n",
    "        max_sequence_length = min(max_sequence_length, max_length)\n",
    "    \n",
    "    # Print word counts and vocabulary size\n",
    "    print(f\"Word Counts: {tokenizer.word_counts}\\n\")\n",
    "    print(f\"Word Index: {tokenizer.word_index}\\nVocabulary Size: {len(tokenizer.word_index)}\\n\")\n",
    "    \n",
    "    # Print example of tokenization\n",
    "    print(f\"Example of Original Text:\\n{X_train.to_list()[0]}\\n\")\n",
    "    print(f\"Example of Tokenized Sequence:\\n{X_train_sequences[0]}\\n\")\n",
    "    \n",
    "    # Print maximum sequence length\n",
    "    print(f\"Maximum sequence length in the dataset: {max_length}\\n\")\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "    X_val_padded = pad_sequences(X_val_sequences, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Print example of padded sequence\n",
    "    print(f\"Example of Padded Sequence:\\n{X_train_padded[0]}\\nLength: {len(X_train_padded[0])}\\n\")\n",
    "    \n",
    "    return X_train_padded, X_val_padded, tokenizer\n",
    "\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "max_vocab_size = 20000\n",
    "max_sequence_length = None\n",
    "\n",
    "X_train_padded, X_val_padded, tokenizer = tokenize_and_pad_sequences(X_train, X_val, max_vocab_size, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/TF-IDF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image4.png](img/tf-idf-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"this is a sample\",\n",
    "    \"this is another example example\",\n",
    "    \"and this is a different sample\"\n",
    "]\n",
    "\n",
    "# Split documents into words\n",
    "tokenized_documents = [doc.split() for doc in documents]\n",
    "\n",
    "# Calculate TF for a term in a specific document\n",
    "def compute_tf(term, document):\n",
    "    term_count = document.count(term)\n",
    "    total_terms = len(document)\n",
    "    return term_count / total_terms\n",
    "\n",
    "# Calculate IDF for a term across all documents\n",
    "def compute_idf(term, all_documents):\n",
    "    num_documents_with_term = sum(1 for doc in all_documents if term in doc)\n",
    "    total_documents = len(all_documents)\n",
    "    return np.log(total_documents / num_documents_with_term)\n",
    "\n",
    "# Calculate TF-IDF for a term in a specific document\n",
    "def compute_tf_idf(term, document, all_documents):\n",
    "    tf = compute_tf(term, document)\n",
    "    idf = compute_idf(term, all_documents)\n",
    "    return tf * idf\n",
    "\n",
    "# Example calculation\n",
    "term = \"example\"\n",
    "document_index = 1  # Document 2\n",
    "document = tokenized_documents[document_index]\n",
    "\n",
    "tf = compute_tf(term, document)\n",
    "idf = compute_idf(term, tokenized_documents)\n",
    "tf_idf = compute_tf_idf(term, document, tokenized_documents)\n",
    "\n",
    "print(f\"TF({term}, Document {document_index + 1}) = {tf}\")\n",
    "print(f\"IDF({term}) = {idf}\")\n",
    "print(f\"TF-IDF({term}, Document {document_index + 1}) = {tf_idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use library TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Natural language processing is fun and interesting\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize and count word frequencies without removing stop words\n",
    "count_vectorizer = CountVectorizer()\n",
    "word_count_matrix = count_vectorizer.fit_transform(texts)\n",
    "word_count_df = pd.DataFrame(word_count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Word Count Matrix:\")\n",
    "print(word_count_df)\n",
    "\n",
    "# Debugging: Print vocabulary to check if \"I\" is included\n",
    "print(\"Vocabulary:\")\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "# Step 2: Inspect the word frequencies\n",
    "word_frequencies = word_count_df.sum(axis=0)\n",
    "sorted_word_frequencies = word_frequencies.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nWord Frequencies:\")\n",
    "print(sorted_word_frequencies)\n",
    "\n",
    "# Step 3: Apply TF-IDF vectorization based on inspected frequencies\n",
    "# Decide the max_features value based on inspection\n",
    "max_features = 10  # Example value, adjust based on your inspection\n",
    "\n",
    "# Initialize TF-IDF vectorizer with chosen max_features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, stop_words=None)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample Data (For demonstration purposes)\n",
    "df_train = pd.DataFrame({\n",
    "    'label': [1, 2, 1, 2],\n",
    "    'text': [\n",
    "        \"I love this product, it's amazing.\",\n",
    "        \"Terrible experience, would not recommend.\",\n",
    "        \"Fantastic quality and great service.\",\n",
    "        \"Worst product ever, completely useless.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "# Show the first text converted to numerical form\n",
    "first_text = X_train.iloc[0]\n",
    "first_text_tfidf = vectorizer.transform([first_text])\n",
    "print(f\"First text: {first_text}\")\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the terms with their corresponding TF-IDF scores for the first text\n",
    "print(\"Terms and TF-IDF scores for the first text:\")\n",
    "for index, score in zip(first_text_tfidf.indices, first_text_tfidf.data):\n",
    "    term = feature_names[index]\n",
    "    print(f\"({index}, {term})\\t{score}\")\n",
    "\n",
    "# Model selection and training\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model evaluation on validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load training data from file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a method for tokenization and word frequency counting\n",
    "def tokenize_and_count_freq(X_train, X_val):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "    X_val_counts = count_vectorizer.transform(X_val)\n",
    "    \n",
    "    word_count_df = pd.DataFrame(X_train_counts.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "    print(\"Word Count Matrix (Training Data):\")\n",
    "    print(word_count_df.head())\n",
    "    \n",
    "    word_frequencies = word_count_df.sum(axis=0)\n",
    "    sorted_word_frequencies = word_frequencies.sort_values(ascending=False)\n",
    "    print(\"\\nWord Frequencies (Training Data):\")\n",
    "    print(sorted_word_frequencies)\n",
    "    \n",
    "    return X_train_counts, X_val_counts, count_vectorizer\n",
    "\n",
    "# Apply tokenization and word frequency counting\n",
    "X_train_counts, X_val_counts, count_vectorizer = tokenize_and_count_freq(X_train, X_val)\n",
    "\n",
    "# Define a method for TF-IDF vectorization\n",
    "def apply_tfidf(X_train, X_val, max_features=None):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    \n",
    "    return X_train_tfidf, X_val_tfidf, tfidf_vectorizer\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "X_train_tfidf, X_val_tfidf, tfidf_vectorizer = apply_tfidf(X_train, X_val, max_features=10000)\n",
    "\n",
    "# Optional: Print the feature names to check\n",
    "print(\"\\nFeature names (TF-IDF):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Optional: Print the first row of the TF-IDF transformed data to check\n",
    "print(\"\\nFirst row of TF-IDF (training data):\")\n",
    "print(X_train_tfidf[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load training data from file\n",
    "def preprocess_data_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        label = int(label.split('__label__')[1])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_from_file(train_filepath)\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[\\W_]+', ' ', text)\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']\n",
    "\n",
    "# Using Keras Tokenizer\n",
    "max_vocab_size = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Calculate maximum sequence length\n",
    "max_sequence_length = max(len(seq) for seq in X_sequences)\n",
    "print(f\"Maximum sequence length in the dataset: {max_sequence_length}\")\n",
    "\n",
    "# Pad sequences\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Split padded sequences and labels into training and validation sets\n",
    "X_train_padded, X_val_padded, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the first padded sequence to check\n",
    "print(\"\\nFirst padded sequence (training data):\")\n",
    "print(X_train_padded[0])\n",
    "\n",
    "# Print the word vocabulary\n",
    "print(\"\\nWord Vocabulary:\")\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# Build and train a simple neural network model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=128, input_length=max_sequence_length),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(64),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "# Function to preprocess and predict sentiment for new texts\n",
    "def preprocess_and_predict(texts, model, tokenizer, max_sequence_length):\n",
    "    texts = [preprocess_text(text) for text in texts]\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    return predictions\n",
    "\n",
    "# Load test data\n",
    "test_filepath = 'test.ft.txt'\n",
    "df_test = preprocess_data_from_file(test_filepath)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)\n",
    "X_test = df_test['text']\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Convert test text to sequences and pad\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"\\nTest accuracy: {test_accuracy}\")\n",
    "\n",
    "# Example prediction\n",
    "new_texts = [\"This is the best book I have ever read!\", \"The movie was too long and boring.\"]\n",
    "predictions = preprocess_and_predict(new_texts, model, tokenizer, max_sequence_length)\n",
    "print(\"\\nPredictions: \", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
