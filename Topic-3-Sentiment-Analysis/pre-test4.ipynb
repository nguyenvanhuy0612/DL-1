{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Load training data from file and preprocess text\n",
    "def preprocess_data_and_text_from_file(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    lines = data.strip().split('\\n')\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        label, text = line.split(' ', 1)\n",
    "        \n",
    "        label = int(label.split('__label__')[1])\n",
    "\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[\\W_]+', ' ', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        text = ' '.join(tokens)\n",
    "\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = 'train.3270.txt'\n",
    "df_train = preprocess_data_and_text_from_file(train_filepath)\n",
    "\n",
    "# Split data into features and labels\n",
    "X = df_train['text']\n",
    "y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a method for TF-IDF vectorization\n",
    "def apply_tfidf(X_train, X_val, max_features=None):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "    \n",
    "    return X_train_tfidf, X_val_tfidf, tfidf_vectorizer\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "X_train_tfidf, X_val_tfidf, tfidf_vectorizer = apply_tfidf(X_train, X_val, max_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature names (TF-IDF):\n",
      "['00' '000' '00290' ... 'zune' 'zydeco' 'zzzzzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "# Optional: Print the feature names to check\n",
    "print(\"\\nFeature names (TF-IDF):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First row of TF-IDF (training data):\n",
      "  (0, 4945)\t0.09079259013672877\n",
      "  (0, 4130)\t0.15530066651714913\n",
      "  (0, 9298)\t0.08617394217254595\n",
      "  (0, 7186)\t0.12932075854994385\n",
      "  (0, 5132)\t0.13363228620606266\n",
      "  (0, 2624)\t0.1006139876021614\n",
      "  (0, 2982)\t0.15304677792952365\n",
      "  (0, 3260)\t0.10996941220802339\n",
      "  (0, 5274)\t0.0894409118909724\n",
      "  (0, 9300)\t0.09272410660580611\n",
      "  (0, 1951)\t0.10996941220802339\n",
      "  (0, 8718)\t0.11542081985817772\n",
      "  (0, 2382)\t0.13270479747446862\n",
      "  (0, 8466)\t0.10307954481532518\n",
      "  (0, 329)\t0.10829155042758754\n",
      "  (0, 2211)\t0.13363228620606266\n",
      "  (0, 7130)\t0.13363228620606266\n",
      "  (0, 2086)\t0.11414224192205502\n",
      "  (0, 5695)\t0.10334085058273854\n",
      "  (0, 5449)\t0.08929538570041912\n",
      "  (0, 1243)\t0.10048230930858489\n",
      "  (0, 1270)\t0.08188361391841693\n",
      "  (0, 4807)\t0.11921259183737265\n",
      "  (0, 9572)\t0.10733915602614436\n",
      "  (0, 4740)\t0.14571225548150182\n",
      "  (0, 4506)\t0.1769690468282356\n",
      "  (0, 5276)\t0.14418605296511966\n",
      "  (0, 8089)\t0.12779455603356168\n",
      "  (0, 7230)\t0.1716921634487071\n",
      "  (0, 9102)\t0.13560408876893063\n",
      "  (0, 5153)\t0.7078761873129424\n",
      "  (0, 1073)\t0.12436130183355026\n",
      "  (0, 3853)\t0.18436841053952904\n"
     ]
    }
   ],
   "source": [
    "# Optional: Print the first row of the TF-IDF transformed data to check\n",
    "print(\"\\nFirst row of TF-IDF (training data):\")\n",
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Build and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = model.predict(X_val_tfidf)\n",
    "print(\"\\nValidation Accuracy: \", accuracy_score(y_val, y_val_pred))\n",
    "print(\"\\nClassification Report: \")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Function to predict sentiment for new texts\n",
    "def predict_sentiment(texts, model, vectorizer):\n",
    "    texts = [preprocess_text(text) for text in texts]\n",
    "    texts = [' '.join(tokens) for tokens in texts]\n",
    "    texts_tfidf = vectorizer.transform(texts)\n",
    "    predictions = model.predict(texts_tfidf)\n",
    "    return predictions\n",
    "\n",
    "# Example prediction\n",
    "new_texts = [\"This is the best book I have ever read!\", \"The movie was too long and boring.\"]\n",
    "predictions = predict_sentiment(new_texts, model, tfidf_vectorizer)\n",
    "print(\"\\nPredictions: \", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
